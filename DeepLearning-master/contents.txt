注：目录是基于《深度学习》的目录起的。基于本项目的内容，目录其实可以分的更细致，这里就分到目录的第三级为止。

**目录**:

- 第二章 线性代数
  - 1 标量, 向量, 矩阵, 张量
  - 2 矩阵转置
  - 3 矩阵加法
  - 4 矩阵乘法
  - 5 单位矩阵
  - 6 矩阵的逆
  - 7 范数
  - 8 特征值分解
  - 9 奇异值分解
  - 10 PCA (主成分分析)


- 第三章 概率与信息论
  - 1 概率
    - 1.1 概率与随机变量
    - 1.2 概率分布
      - 1.2.1 概率质量函数
      - 1.2.2 概率密度函数
      - 1.2.3 累积分布函数
    - 1.3 条件概率与条件独立
    - 1.4 随机变量的度量
    - 1.5 常用概率分布
      - 1.5.1 伯努利分布 (两点分布)
      - 1.5.2 范畴分布 (分类分布)
      - 1.5.3 高斯分布 (正态分布)
      - 1.5.4 多元高斯分布 (多元正态分布)
      - 1.5.5 指数分布
      - 1.5.6 拉普拉斯分布
      - 1.5.7 Dirac 分布
    - 1.6 常用函数的有用性质
      - 1.6.1 logistic sigmoid 函数
      - 1.6.2 softplus 函数
  - 2 信息论
  - 3 图模型
    - 3.1 有向图模型
      - 3.1.1 贝叶斯网的独立性
    - 3.2 无向图模型
      - 3.1.2 马尔可夫网的条件独立性


- 第四章 数值计算
  - 1 上溢和下溢
  - 2 优化方法
    - 2.1 梯度下降法
    - 2.2 牛顿法
    - 2.3 约束优化


- 第五章 机器学习基础
  - 1 学习算法
    - 1.1 举例:线性回归 
  - 2 容量、过拟合、欠拟合
    - 2.1 泛化问题
    - 2.2 容量
  - 3 超参数与验证集
  - 4 偏差和方差
    - 4.1 偏差
    - 4.2 方差
    - 4.3 误差与偏差和方差的关系
  - 5 最大似然估计
  - 6 贝叶斯统计
  - 7 最大后验估计
    - 7.1 举例:线性回归
  - 8 监督学习方法
    - 8.1 概率监督学习
    - 8.2 支持向量机
      - 8.2.1 核技巧
    - 8.3 k-近邻
    - 8.4 决策树
      - 8.4.1 特征选择
      - 8.4.2 决策树生成
      - 8.4.3 决策树正则化
  - 9 无监督学习方法
    - 9.1 主成分分析法
    - 9.2 k-均值聚类


- 第六章 深度前馈网络
  - 1 深度前馈网络
  - 2 DFN 相关设计
    - 2.1 隐藏单元
    - 2.2 输出单元
    - 2.3 代价函数
    - 2.4 架构设计
  - 3 反向传播算法
    - 3.1 单个神经元的训练
    - 3.2 多层神经网络的训练
      - 3.2.1 定义权重初始化方法
      - 3.2.2 定义激活函数
      - 3.2.3 定义优化方法
      - 3.2.4 定义网络层的框架
      - 3.2.5 定义代价函数
      - 3.2.6 定义深度前馈网络
  - 4 神经网络的万能近似定理
  - 5 实例:学习 XOR


- 第七章 深度学习中的正则化
  - 1 参数范数惩罚
    - 1.1 L2 正则化
    - 1.2 L1 正则化
    - 1.3 总结 (L2 正则化与L1 正则化的解)
    - 1.4 作为约束的范数惩罚
    - 1.5 欠约束问题
  - 2 数据增强
    - 2.1 数据集增强
    - 2.2 噪声鲁棒性
  - 3 训练方案
    - 3.1 半监督学习
    - 3.2 多任务学习
    - 3.3 提前终止
  - 4 模型表示
    - 4.1 参数绑定与共享
    - 4.2 稀疏表示
    - 4.3 Bagging 及其他集成方法
      - 4.3.1 Bagging 方法
      - 4.3.2 随机森林
      - 4.3.3 方法解决过拟合
    - 4.4 Dropout
  - 5 样本测试
  - 6 补充材料
    - 6.1 Boosting
      - 6.1.1 前向分步加法模型
      - 6.1.2 AdaBoost 算法
      - 6.1.3 Boosting Tree 算法与 GBDT 算法
      - 6.1.4 XGBoost 算法


- 第八章 深度模型中的优化
  - 1 基本优化算法
    - 1.1 梯度
      - 1.1.1 梯度下降
      - 1.1.2 随机梯度下降
    - 1.2 动量
      - 1.2.1 Momentum 算法
      - 1.2.2 NAG 算法
    - 1.3 自适应学习率
      - 1.3.1 AdaGrad 算法
      - 1.3.2 RMSProp 算法
      - 1.3.3 AdaDelta 算法
      - 1.3.4 Adam 算法
    - 1.4 二阶近似方法
      - 1.4.1 牛顿法
      - 1.4.2 拟牛顿法
  - 2 优化策略
    - 2.1 参数初始化
  - 3 批标准化
  - 4 坐标下降
  - 5 Polyak 平均
  - 6 监督预训练
  - 7 设计有助于优化的模型


- 第九章 卷积网络
  - 1 卷积运算
  - 2 池化
  - 3 深度学习框架下的卷积
    - 3.1 多个并行卷积
    - 3.2 输入值与核
    - 3.3 填充 (Padding)
    - 3.4 卷积步幅 (Stride)
  - 4 更多的卷积策略
    - 4.1 深度可分离卷积 (Depthwise Separable Convolution)
    - 4.2 分组卷积 (Group Convolution)
    - 4.3 扩张卷积 (Dilated Convolution)
  - 5 GEMM 转换
  - 6 卷积网络的训练
    - 6.1 卷积网络示意图
    - 6.2 单层卷积层/池化层
      - 6.2.1 卷积函数的导数及反向传播
      - 6.2.2 池化函数的导数及后向传播
    - 6.3 多层卷积层/池化层
    - 6.4 Flatten 层 & 全连接层
  - 7 平移等变
  - 8 代表性的卷积神经网络
    - 8.1 卷积神经网络 (LeNet)


- 第十一章 实践方法论
  - 1 实践方法论
  - 2 性能度量指标
    - 2.1 错误率与准确性
    - 2.2 查准率、查全率与 F1 值
      - 2.2.1 混淆矩阵
      - 2.2.2 查准率和查全率的定义与关联
      - 2.2.3 F1 值
    - 2.3 PR 曲线
    - 2.4 ROC 曲线与 AUC 值
      - 2.4.1 ROC 曲线
      - 2.4.2 AUC 值的计算方法
    - 2.5 覆盖
    - 2.6 指标性能的瓶颈
  - 3 默认基准模型
  - 4 确定是否收集更多数据
  - 5 选择超参数
    - 5.1 手动超参数调整
    - 5.2 自动超参数优化算法
      - 5.2.1 网格搜索 (Grid Search)
      - 5.2.2 随机搜索 (Random Search)
      - 5.2.3 基于模型的超参数优化 (Model-based Hyperparameter Optimization)
